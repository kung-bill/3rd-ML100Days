{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "清楚了解 L1, L2 的意義與差異為何，並了解 LASSO 與 Ridge 之間的差異與使用情境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請閱讀相關文獻，並回答下列問題\n",
    "\n",
    "[脊回歸 (Ridge Regression)](https://blog.csdn.net/daunxx/article/details/51578787)\n",
    "[Linear, Ridge, Lasso Regression 本質區別](https://www.zhihu.com/question/38121173)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. LASSO 回歸可以被用來作為 Feature selection 的工具，請了解 LASSO 模型為什麼可用來作 Feature selection\n",
    "> LASSO是用 L1 norm 當作 penalty function 的線性回歸算法。 <br>\n",
    "> 觀察L1 norm 的在2D平面上的幾何解釋發現，L1 的最小值通常出現在菱形的角點。 加入了L1 norm penalty 解出來的迴歸係數大部分的值會被\"強迫\"變成0，這樣的的係數有 sparse 的性質，這樣的性質能凸顯出重要的係數，等於0的係數將無法影響預測結果，只有少數非0的係數能影響預測結果。 <br>\n",
    "> 綜合以上討論，LASSO 可以拿來做 Feature selection。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 當自變數 (X) 存在高度共線性時，Ridge Regression 可以處理這樣的問題嗎?\n",
    "> 樣本間存在高度共線性，用數學特性來刻劃就是樣本矩陣的 condition number 很大，只要 weight 有微小的變化，預測值就有很大的改變。<br>\n",
    "> 有這種性質的矩陣很接近 singular matrix ，在數值上 condition number 很大的矩陣求逆是 ill-posed 的問題。 <br>\n",
    "> **Linear Regression:** <br>\n",
    "> $$\\hat{w} = \\arg \\min_{w} \\| y - X w\\|_2^2 ,$$\n",
    "> where $  y \\in \\mathbb{R}^{n \\times 1}, X \\in \\mathbb{R}^{n \\times N} \\text{ and } w \\in \\mathbb{R}^{N \\times 1}  (n<<N).$ <br>\n",
    "> 對向量 $w$ 微分，取得close form： <br>\n",
    "> $$\\hat{w} = (X^\\top X)^{-1}X^\\top y \\tag{1}$$\n",
    "> $(X^\\top X)$ 的 condtion number $\\kappa(X^\\top X)$ 滿足以下等式 [可參考]( https://math.stackexchange.com/questions/1351616/condition-number-of-ata) ： <br>\n",
    "> $$\\kappa(X^\\top X) = \\kappa(X)^2$$ \n",
    "> 考慮 **Redige Regression:** <br>\n",
    "> $$\\hat{w} = \\arg \\min_{w} \\| y - X w\\|_2^2 + \\lambda \\| w \\|_2^2 ,$$\n",
    "> where $\\lambda > 0.$ (ridge parameter).<br>\n",
    "> 其 close form: <br>\n",
    "> $$\\hat{w} = (X^\\top X + \\lambda I)^{-1}X^\\top y $$\n",
    "> $(X^\\top X + \\lambda I)$ 的 condition number 會比  $(X^\\top X)$ 還小，因為 $I$ 是一個nonsingular matrix，如果 $\\lambda$ 夠大，可讓 $(X^\\top X + \\lambda I)$ 每個column 之間更加線性無關。<br>\n",
    "> 這種 L2 regularization 也有人稱為 Tikhonov regularization。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
