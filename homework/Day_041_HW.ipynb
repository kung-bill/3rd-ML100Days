{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "\n",
    "\n",
    "\n",
    "閱讀以下兩篇文獻，了解決策樹原理，並試著回答後續的問題\n",
    "- [決策樹 (Decision Tree) - 中文](https://medium.com/@yehjames/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)\n",
    "- [how decision tree works - 英文](http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/)\n",
    "\n",
    "1. 在分類問題中，若沒有任何限制，決策樹有辦法在訓練時將 training loss 完全降成 0 嗎？\n",
    "2. 決策樹只能用在分類問題嗎？還是可以用來解決回歸問題？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 在分類問題中，若沒有任何限制，決策樹有辦法在訓練時將 training loss 完全降成 0 嗎？\n",
    "> 可以，決策樹模型的Training 透過：<br>\n",
    "> 1. 特徵選擇 \n",
    "> 2. 決策樹構建\n",
    "> 3. 剪枝 <br> \n",
    "> <br>\n",
    "> 決策樹模型很容易太過複雜，對於Training set 中每一個樣本都分得很清楚。將每一個特例，都產生一個新的Internal Node 將其分清楚，也就是 Overfitting。可透過剪枝與限制樹的高度來減少過擬合的情況。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 決策樹只能用在分類問題嗎？還是可以用來解決回歸問題？\n",
    "> 決策樹可以用在分類與回歸，決策樹用在回歸問題上，也稱為回歸樹，回歸樹的特別之處為，每一個Internal Node，所產生的分割，並不像分類樹，尋找讓 Entropy 或 Gini-index 最大的分割，而是用 MSE 來評斷： <br>\n",
    "> $$MSE (t) = \\frac{1}{N_t} \\sum_{i \\in D_t}(y^{(i)} - \\hat{y}_t)$$\n",
    "> 這裡的 $N_t$ 為第 $t$ 組分割內的所有樣本總數，$D_t$ 是第 $t$ 組分割的子集合，$\\hat{y}_t$ 用的是該分割中所有的目標值 $y^{(i)}$的平均，因為現在的目標值不再是categories，所以可以計算其平均數:\n",
    "> $$\\hat{y}_t = \\frac{1}{N_t} \\sum_{i \\in D_t} y^{(i)}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
